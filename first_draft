def remove_html(url):
    content = request.urlopen(url).read().decode('utf8')
    content = html.unescape(content)
    html_meta_data= re.compile(r'''(?x)             #compiling unwanted strings
    [Â¶|]
    |<.*?>
    |<a\s+\S*
    |<!\S*
    |\S+\/\/\S+
    |>(?:modules|index|next)
    |(http
    |html
    |HTML
    |Module\s)\S*
    |(?:Index|News)"
    |(?:inter|exter)nal\S*"
    |(?:class|href|title|rel|accesskey)="\S*''')
    return html_meta_data.sub('', content)


def sentence_positon(article_string):
    #sentence tokenize the string
    return #a list of tuples (sent, positin)


def word_tf_(article_string):
    #split the string
    #frequency distribution of each word_tf
    #dictionary of words and their frequency
    #devide each word count by the total number of words
    return #a dictionary of words and their tf scores


# we need a whole class for idf
def word_idf(article_string):
    #split the string
    #length of the list= number of words
    #idf= log (number of documents/the number of documents with the specific word)
    return #a dictonary of words and their idf scores


def tf_idf_scores(word_tf_dict, word_idf_dict):
    #idf= word[tf] * word[idf]
    #create a dictionary of words and tf_idf_scores
    #sort them by value
    return #a list of 5 keywords


def build_summary(article_string, keyword_lst):
    #tokenize the string into a list of sents
    # n =the length of that list/10
    #pick n sents that have most of the keywords


if __name__ == '__main__':
    article= remove_html('https://www.nytimes.com/2018/12/02/business/trade-truce-china-us.html')
